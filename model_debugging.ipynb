{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import json\n",
    "import pickle\n",
    "from cogworks_data.language import get_data_path\n",
    "from typing import List, Union, Sequence\n",
    "from collections import Counter\n",
    "from pathlib import Path\n",
    "import os\n",
    "from pathlib import Path\n",
    "import json\n",
    "from tokenizer import process_caption\n",
    "from resnet_loading import resnet18_features\n",
    "\n",
    "from gensim.models import KeyedVectors\n",
    "from coco_data import COCODataManager\n",
    "from operator import itemgetter\n",
    "\n",
    "from train_model import train_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"glove.6B.200d.txt.w2v\"\n",
    "### this takes a while to load -- keep this in mind when designing your capstone project ###\n",
    "glove = KeyedVectors.load_word2vec_format(get_data_path(filename), binary=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_coco_data():\n",
    "    \"\"\"\n",
    "    coco_data can be used using the following:\n",
    "    \n",
    "    coco_data = initialize_coco_data()\n",
    "    image_ids = coco_data.get_image_ids()\n",
    "    captions = coco_data.get_caption_ids()\n",
    "    image_to_captions = coco_data.image_to_caption()\n",
    "    caption_id_to_images = coco_data.caption_id_to_image()\n",
    "    caption_id_to_captions = coco_data.caption_id_to_caption()\n",
    "    \"\"\"\n",
    "    filename = get_data_path(\"captions_train2014.json\")\n",
    "    coco_data = COCODataManager(filename)\n",
    "    return coco_data\n",
    "\n",
    "coco_data = initialize_coco_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def se_text(text: str, captions: Sequence[str]) -> np.ndarray: # um someone who has taken more math than algebra II please check this lol\n",
    "    \"\"\"Takes text and returns a shape (200,) array by using IDF and glove embeddings.\n",
    "    \"\"\"\n",
    "    global glove \n",
    "    text_tokens = process_caption(text) # len N\n",
    "    caption_tokens = [process_caption(cap) for cap in captions]\n",
    "\n",
    "    total_tokens = [token for cap in captions for token in process_caption(cap)] + text_tokens\n",
    "\n",
    "    vocab = set(total_tokens)\n",
    "    counters = []\n",
    "    for caption_token in caption_tokens:\n",
    "        counters.append(Counter(caption_token))\n",
    "    counters.append(Counter(text_tokens))\n",
    "\n",
    "    N = len(counters)\n",
    "    nt = [sum(1 if t in counter else 0 for counter in counters) for t in text_tokens]\n",
    "    nt = np.array(nt, dtype=float)\n",
    "    idf = np.log10(N / nt) # shape (N,)\n",
    "\n",
    "    glove_embeddings = []\n",
    "    for word in text_tokens:\n",
    "        if word in glove:\n",
    "            glove_embeddings.append(glove[word]) # append glove embedding if glove contains word, shape (200,)\n",
    "        else:\n",
    "            glove_embeddings.append(np.zeros(shape=(200,))) # else append array of zeros shape (200,)\n",
    "\n",
    "    glove_embeddings = np.array(glove_embeddings) # shape (N, 200)\n",
    "    for i, weight in enumerate(idf):\n",
    "        glove_embeddings[i] += weight\n",
    "\n",
    "    ret = glove_embeddings / np.linalg.norm(glove_embeddings) # shape (N, 200)\n",
    "    return ret.mean(axis=0) # should be shape (200,)? hopefully??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_training_tuples():\n",
    "    # training tuples should have:\n",
    "    # (img_descriptor, semantic embedding of descriptor's caption, semantic embedding of diff img caption)\n",
    "    # image ids come from resnet\n",
    "\n",
    "    print(len(resnet18_features)) # 82612\n",
    "\n",
    "    idxs = np.arange(len(resnet18_features)) #np.arange(15) \n",
    "    np.random.shuffle(idxs)\n",
    "\n",
    "    print(len(idxs))\n",
    "\n",
    "    training_idxs = idxs[0:len(resnet18_features)*3//4] # for even splitting purposes, 3/4 train, 1/4 test\n",
    "    training_ids = [list(resnet18_features.keys())[key_idx] for key_idx in training_idxs]\n",
    "    # ^ do the whole list(keys) thing bc dicts don't have indexes to access xyz elements,\n",
    "    # so may need to make that a global variable to make sure the training/test don't \n",
    "    # overlap\n",
    "\n",
    "    training_descriptor_vectors = np.asarray(itemgetter(*training_ids)(resnet18_features))\n",
    "\n",
    "    #coco_data = initialize_coco_data()\n",
    "    \n",
    "    print(type(itemgetter(*training_ids)(coco_data.image_id_to_captions)), type(itemgetter(*training_ids)(coco_data.image_id_to_captions)[0]))\n",
    "    caption_ids = np.asarray(itemgetter(*training_ids)(coco_data.image_id_to_captions), dtype = object)[:, 0]\n",
    "\n",
    "    print(caption_ids.shape, type(caption_ids[0]))\n",
    "    #print(caption_ids[0])\n",
    "\n",
    "    #cap_slice = caption_ids[:, 0]\n",
    "\n",
    "    #print(cap_slice)\n",
    "    '''print(type(training_ids), type(caption_ids), type(coco_data.caption_id_to_captions))\n",
    "    print(caption_ids.shape, caption_ids[:, 0].shape)\n",
    "    print(\"wait\")\n",
    "    print(caption_ids[0][0])\n",
    "    print(\"wait\")\n",
    "    print(len(caption_ids), len(caption_ids[0]))\n",
    "    print(itemgetter(*caption_ids)(coco_data.caption_id_to_captions))'''\n",
    "    text_captions = np.asarray(itemgetter(*caption_ids)(coco_data.caption_id_to_captions))\n",
    "    caption_to_embeddings = {caption : se_text(caption, text_captions) for caption in text_captions}\n",
    "\n",
    "    good_image_embeddings = np.asarray(itemgetter(*text_captions)(caption_to_embeddings))\n",
    "\n",
    "    np.random.shuffle(text_captions) # so that we dont cut into testing data\n",
    "    bad_image_embeddings = np.asarray(itemgetter(*text_captions)(caption_to_embeddings)) # now train idxs is idxs for bad img embeddings\n",
    "\n",
    "    return training_descriptor_vectors, good_image_embeddings, bad_image_embeddings\n",
    "\n",
    "make_training_tuples()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "week3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
